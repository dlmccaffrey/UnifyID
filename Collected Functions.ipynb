{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model A: Centroid Anomaly Detection\n",
    "The first model is a Centroid Amonaly Detection algorithm. See ```Method 1 - Centroid Based Algorithm.ipynb``` for a more detailed explanation of the algorithm, as well as development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_A_initialize(X_train):\n",
    "    '''\n",
    "    This function initializes the centroid anomaly detection algorithm. It makes sure that the initial data is mean\n",
    "    normalized, which makes calculating the centroid trivial (it is zeros). It calculates the distance each data point\n",
    "    is from the centroid and uses the standard devation of the distances to calculate r, the threshold distance for\n",
    "    classifying points as outliers or inliers.\n",
    "    \n",
    "    Inputs:\n",
    "        X_train: a DataFrame of features. All features will be used in the algorithm.\n",
    "        \n",
    "    Outputs:\n",
    "        centroid: the centroid of the training data, an array of zeros\n",
    "        r: the threshold distance from the centroid\n",
    "        n: the number of training datapoints\n",
    "    '''\n",
    "    #imports\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    #mean normalize the input\n",
    "    X_train = (X_train - X_train.mean()) / (X_train.std())\n",
    "\n",
    "    # the centroid should be zero, so I'm hardcoding that to avoid rounding errors\n",
    "    centroid = np.zeros(X_train.shape[1])\n",
    "    \n",
    "    # calculate r from data\n",
    "    train_distances = np.linalg.norm(X_train, axis=1) #centroid is zero, so distance is just the norm\n",
    "    multiplier = 2.9 #determined through validation\n",
    "    sigma = train_distances.std() #the standard deviation of the distances\n",
    "    r = multiplier * sigma #the distance threshold for outliers\n",
    "    \n",
    "    n = X_train.shape[0]\n",
    "    \n",
    "    return centroid, r, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_A_update(X_update, centroid, r, n):\n",
    "    '''\n",
    "    This function updates the centroid model based on new data in X_update. First, it predicts which data points are\n",
    "    outliers. It does this by calculating the distance of the points from the centroid. If the distance is greater than\n",
    "    or equal to the threshold, r, then the point is an outlier. If a majority of the points are not outliers, the\n",
    "    centroid gets updated.\n",
    "    \n",
    "    Inputs:\n",
    "        X_update: a DataFrame of features. All features will be used in the algorithm.\n",
    "        \n",
    "    Outputs:\n",
    "        centroid: the centroid of the training data\n",
    "        r: the threshold distance from the centroid\n",
    "        n: the number of training datapoints\n",
    "        y_predicted: returns the predictions for the data point. 0=inlier, 1=outlier\n",
    "    '''\n",
    "    #imports\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    #make sure X_update is mean normalized\n",
    "    X_update = (X_update - X_update.mean()) / (X_update.std())\n",
    "\n",
    "    # first, see if majority of the points are predicted to be outliers\n",
    "    update_distances = np.linalg.norm(X_update - centroid, axis=1)\n",
    "    y_predicted = np.zeros(X_update.shape[0]) #start with zeros\n",
    "    y_predicted[np.where(update_distances >= r)] = 1 #change to 1's where >= r\n",
    "    frac_outliers = y_predicted.sum() / y_predicted.shape[0] #calculate fraction of outliers\n",
    "\n",
    "    if frac_outliers < 0.5: #if a majority of the points are not outliers\n",
    "        #update the model\n",
    "        for row in X_update.iterrows():\n",
    "            centroid = centroid + (1 / n) * (row[1].values - centroid)\n",
    "            \n",
    "    return centroid, r, n, y_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model B: One Class Support Vector Machine\n",
    "The second model is a one class SVM algorithm. See ```Method 2 - One Class SVM.ipynb``` for a more detailed explanation of the algorithm, as well as development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_B_initialize(X_train):\n",
    "    \"\"\"\n",
    "    This algorithm implements the inital training for the OneClassSVM() algorithm from scikit learn. Hyperparameters\n",
    "    were optimized previously to nu = 0.1 and kernel = 'linear'.\n",
    "    \n",
    "    Inputs:\n",
    "        X_train: A DataFrame of features. All features are input into the model.\n",
    "        \n",
    "    Output:\n",
    "        model: The OneClassSVM() model object with stored fit results.\n",
    "        n_support_vectors: The number of suport vectors used by the algorithm.\n",
    "    \"\"\"\n",
    "    #imports\n",
    "    import pandas as pd\n",
    "    from sklearn import svm\n",
    "    \n",
    "    #make sure X_train is mean normalized\n",
    "    X_train = (X_train - X_train.mean()) / (X_train.std())\n",
    "\n",
    "    # create model\n",
    "    model = svm.OneClassSVM(nu=0.1, kernel='linear') #hyperparameters found through validation\n",
    "\n",
    "    # fit the model\n",
    "    model.fit(X_train)\n",
    "    \n",
    "    #extract the number of support vectors\n",
    "    support_vectors = model.support_vectors_\n",
    "    n_support_vectors = support_vectors.shape[0]\n",
    "\n",
    "    return model, n_support_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_B_update(X_update, model, n_support_vectors):\n",
    "    \"\"\"\n",
    "    This function updates the OneClassSVM() algorithm with new data. It uses the support vectors from the previous\n",
    "    model and the new data to train a new model. This data set is much smaller than the original, so training is \n",
    "    faster. nu is also set to be n_support_vectors / (n_support_vectors + n_new_data) so that the number of support\n",
    "    vectors stays roughly constant. n_support_vectors is held constant across updates, but the support vectors are\n",
    "    updated every time.\n",
    "    \n",
    "    Inputs:\n",
    "        X_update: a DataFrame of features. All features will be used in the algorithm.\n",
    "        model: The OneClassSVN() model object from the previous training. The model must include fit results.\n",
    "        n_support_vectors: The number of support vectors generated in the initial training\n",
    "        \n",
    "    Outputs:\n",
    "        model_update: The updated OneClassSVM() model object with stored fit results.\n",
    "        y_predict: The predictions for each data point. 0=normal, 1=outlier.\n",
    "    \"\"\"\n",
    "    #imports\n",
    "    import pandas as pd\n",
    "    from sklearn import svm\n",
    "    \n",
    "    #extract the support vectors\n",
    "    support_vectors = model.support_vectors_\n",
    "\n",
    "    X_update = (X_update - X_update.mean()) / (X_update.std())\n",
    "\n",
    "    #see how many are outliers\n",
    "    y_predict = model.predict(X_update)\n",
    "\n",
    "    # OneClassSVM() outputs 1 for outliers and -1 for normal\n",
    "    y_predict[y_predict==-1] = 0\n",
    "    frac_outliers = y_predict.sum() / y_predict.shape[0]\n",
    "\n",
    "    if frac_outliers < 0.5:\n",
    "        n_new_data = X_update.shape[0]\n",
    "\n",
    "        #combine the support vectors and the new data points\n",
    "        X_new = np.vstack((support_vectors, X_update.values))\n",
    "\n",
    "        #recalculate nu\n",
    "        nu = n_support_vectors / (n_support_vectors + n_new_data)\n",
    "\n",
    "        #fit the new data\n",
    "        model_update = svm.OneClassSVM(nu=nu, kernel='linear')\n",
    "        model_update.fit(X_new)\n",
    "        \n",
    "    return model_update, y_predict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
