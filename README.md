# Online Outlier Detection Challenge for UnifyID

I used the <a href="https://www.kaggle.com/dalpozz/creditcardfraud">Kaggle Credit Card Fraud Detection</a> dataset. ```Data Exploration and Setup.ipynb``` contains the code I used for exploring the dataset a bit and also how I divided it to use in my models. The data files that I created have not been pushed to GitHub.

The first method I looked at was the centroid anomaly detection algorithm from Kloft M, Laskov P. <a href="http://proceedings.mlr.press/v9/kloft10a.html">Online Anomaly Detection under Adversarial Impact</a>. Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. 2010. pp. 405–412. It's a simple model that doesn't require much computational power. It looks at the Euclidean distance between data points and the centroid (mean) value of the training dataset. Any points past a certain distance are considered outliers. I was able to achieve 90.9% AUC with this algorithm.

The second method I looked at was the one class Support Vector Machine, based on Schölkopf B, Williamson RC, Smola AJ, Shawe-Taylor J, Platt JC. <a href="https://www.researchgate.net/publication/221619107_Support_Vector_Method_for_Novelty_Detection">Support Vector Method for Novelty Detection</a>. Advances in Neural Information Processing Systems 12, [NIPS Conference, Denver, Colorado, USA, November 29 - December 4, 1999]. 1999. pp. 582–588. In essence, this algorithm treats the origin in feature space as the second class and creates an appropriate decision boundary. This model is not necessarily the best for the Kaggle dataset, but it seems like a valuable model to know for real world datasets, which likely have > 100 features. I used the ```svm.OneClassSVM()``` implementation in scikit learn, with my own addition to the algorithm for making updates. I achieved 80.5% AUC.

For convenience, the final functions have been collected in the notebook ```Collected Fucntions.ipynb```.

## Future Directions
Since there was no way to know how meaningful the 'Time' feature was, I ignored it. It is possible that treating the data as a time series could improve the accuracy based on when the fraudulent transactions happened, specifically if there are several in a short time period.

There are also many more models to try. I found a review <a href="http://www.sciencedirect.com/science/article/pii/S016516841300515X">here</a> that is a gold mine of ideas. Specifically, I'm most familiar with neural networks, so I would like to try a replicator neural network based on Hawkins S, He H, Williams G, Baxter R. <a href="https://link.springer.com/chapter/10.1007/3-540-46145-0_17">Outlier Detection Using Replicator Neural Networks</a>. Data Warehousing and Knowledge Discovery. Springer, Berlin, Heidelberg; 2002. pp. 170–180. The input and the output layers are the same, so the algorithm measures the error of trying to reconstruct the inputs. Any future inputs that it fails to reconstruct are considered outliers. The appealing part of this algorithm is that the updates are algorithmically easy and make sense conceptually.